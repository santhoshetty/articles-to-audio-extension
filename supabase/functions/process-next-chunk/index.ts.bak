import { serve } from "https://deno.land/std@0.168.0/http/server.ts"
import { OpenAI } from "https://esm.sh/openai@4.28.0"
import { createClient } from "https://esm.sh/@supabase/supabase-js@2.39.3"

serve(async (req) => {
  try {
    // Log request received
    console.log("Process next chunk function called")
    
    // Get the job ID and next chunk index from the request body
    const { jobId, nextChunkIndex } = await req.json()
    
    if (!jobId) {
      console.error("Missing job ID")
      throw new Error("Job ID is required")
    }

    if (nextChunkIndex === undefined) {
      console.error("Missing next chunk index")
      throw new Error("Next chunk index is required")
    }

    // Initialize OpenAI with the API key from edge function secrets
    const apiKey = Deno.env.get('openai_api_key')
    if (!apiKey) {
      console.error("OpenAI API key not found in environment")
      throw new Error("OpenAI API key not configured")
    }

    const openai = new OpenAI({ apiKey })

    // Get Supabase clients
    const supabaseUrl = Deno.env.get('SUPABASE_URL')
    const supabaseServiceKey = Deno.env.get('SUPABASE_SERVICE_ROLE_KEY')
    
    if (!supabaseUrl || !supabaseServiceKey) {
      console.error("Missing Supabase configuration")
      throw new Error("Supabase configuration is incomplete")
    }

    // Use service role for this function since it's called from another edge function
    const supabaseAdmin = createClient(
      supabaseUrl,
      supabaseServiceKey
    )

    // Get the chunk text and job info
    const { data: chunkData, error: chunkError } = await supabaseAdmin
      .from('podcast_chunks')
      .select('chunk_text, job_id')
      .eq('job_id', jobId)
      .eq('chunk_index', nextChunkIndex)
      .single()

    if (chunkError || !chunkData) {
      console.error(`Error fetching chunk ${nextChunkIndex}:`, chunkError)
      throw new Error(`Failed to get chunk data: ${chunkError?.message || "Chunk not found"}`)
    }

    // Get user ID from the job
    const { data: jobData, error: jobError } = await supabaseAdmin
      .from('podcast_jobs')
      .select('user_id')
      .eq('id', jobId)
      .single()

    if (jobError || !jobData) {
      console.error("Error fetching job:", jobError)
      throw new Error(`Failed to get job data: ${jobError?.message || "Job not found"}`)
    }

    const userId = jobData.user_id

    // Update chunk status to processing
    await supabaseAdmin
      .from('podcast_chunks')
      .update({
        status: 'processing'
      })
      .eq('job_id', jobId)
      .eq('chunk_index', nextChunkIndex)

    console.log(`Processing chunk ${nextChunkIndex} for job ${jobId}...`)

    try {
      // Clean the input text - ensure newlines are properly handled
      const cleanedText = chunkData.chunk_text.replace(/\\n/g, "\n").trim()
      
      // Split the cleaned text into speaker segments
      const segments = []
      const lines = cleanedText.split('\n')
      
      let currentSpeaker = null
      let currentSegment = ""
      
      for (const line of lines) {
        const trimmedLine = line.trim()
        if (!trimmedLine) continue
        
        // Check if this is a new speaker
        const aliceMatch = trimmedLine.match(/^Alice:/)
        const bobMatch = trimmedLine.match(/^Bob:/)
        
        if (aliceMatch || bobMatch) {
          // If we have a previous segment, push it
          if (currentSegment && currentSpeaker) {
            segments.push({
              text: currentSegment.trim(),
              speaker: currentSpeaker
            })
          }
          
          // Start a new segment
          currentSpeaker = aliceMatch ? "Alice" : "Bob"
          currentSegment = trimmedLine
        } else if (currentSpeaker) {
          // Continue current segment
          currentSegment += " " + trimmedLine
        }
      }
      
      // Add the last segment if exists
      if (currentSegment && currentSpeaker) {
        segments.push({
          text: currentSegment.trim(),
          speaker: currentSpeaker
        })
      }
      
      console.log(`Split chunk ${nextChunkIndex} into ${segments.length} speaker segments`)
      
      // Generate audio for each segment
      const audioBuffers = []
      const audioMetadata = [] // Keep track of which voice was used for each segment
      
      for (let i = 0; i < segments.length; i++) {
        const segment = segments[i]
        // Clean the segment text by removing the speaker prefix
        const cleanedSegmentText = segment.text.replace(/^(Alice|Bob):\s*/i, "").trim()
        
        if (cleanedSegmentText.length === 0) continue
        
        // Check if segment is within TTS limit
        if (cleanedSegmentText.length > 4000) {
          console.warn(`Segment ${i} for speaker ${segment.speaker} exceeds 4000 characters (${cleanedSegmentText.length}), truncating`)
          // Truncate to stay under limits
          segment.text = cleanedSegmentText.substring(0, 3900) + "..."
        }
        
        // Select the appropriate voice based on speaker
        const voice = segment.speaker === "Alice" ? "alloy" : "onyx"
        
        console.log(`Generating audio for ${segment.speaker} segment ${i} with voice ${voice} (${cleanedSegmentText.length} chars)`)
        
        // Special handling for segment 5 which has been causing issues
        if (i === 5) {
          console.log("Special handling for segment 5 which has caused issues in the past")
          try {
            // More aggressive text cleaning for segment 5
            const extraSafeText = cleanedSegmentText
              .replace(/[^\x20-\x7E\s]/g, "") // Remove non-printable ASCII characters
              .replace(/(\r\n|\n|\r)/gm, " ") // Replace newlines with spaces
              .replace(/\s+/g, " ") // Replace multiple spaces with single space
              .replace(/[""]/g, "\"") // Replace smart quotes with straight quotes
              .replace(/['']/g, "'") // Replace smart apostrophes with straight ones
              .replace(/[–—]/g, "-") // Replace em/en dashes with hyphens
              .replace(/[…]/g, "...") // Replace ellipsis character with periods
              .replace(/&/g, " and ") // Replace ampersands
              .replace(/[^a-zA-Z0-9.,?!'" ]/g, "") // Even more aggressive filtering - alphanumeric and basic punctuation only
              .trim()
            
            // For segment 5, use a different approach - break into very small chunks
            const maxChunkSize = 500; // Very conservative size
            let remainingText = extraSafeText;
            let chunkIndex = 0;
            
            while (remainingText.length > 0 && chunkIndex < 10) { // Limit to 10 chunks for safety
              // Find a sentence boundary to split on
              let chunkEndIndex = Math.min(maxChunkSize, remainingText.length);
              if (chunkEndIndex < remainingText.length) {
                // Look for sentence boundary
                const periodIndex = remainingText.lastIndexOf('.', chunkEndIndex);
                const questionIndex = remainingText.lastIndexOf('?', chunkEndIndex);
                const exclamationIndex = remainingText.lastIndexOf('!', chunkEndIndex);
                
                // Find the last sentence boundary in our window
                const boundaryIndex = Math.max(
                  periodIndex > 0 ? periodIndex : -1,
                  questionIndex > 0 ? questionIndex : -1,
                  exclamationIndex > 0 ? exclamationIndex : -1
                );
                
                if (boundaryIndex > 0) {
                  chunkEndIndex = boundaryIndex + 1; // Include the punctuation
                }
              }
              
              const currentChunk = remainingText.substring(0, chunkEndIndex);
              remainingText = remainingText.substring(chunkEndIndex);
              
              console.log(`Processing segment 5 sub-chunk ${chunkIndex}: ${currentChunk.length} chars`);
              
              try {
                const speechResponse = await openai.audio.speech.create({
                  model: "tts-1",
                  voice: voice,
                  input: currentChunk,
                });
                
                const audioBuffer = await speechResponse.arrayBuffer();
                console.log(`Successfully decoded audio response for segment 5 sub-chunk ${chunkIndex}, size: ${audioBuffer.byteLength} bytes`);
                audioBuffers.push(audioBuffer);
                audioMetadata.push({ speaker: segment.speaker, voice });
                
                chunkIndex++;
                // Small delay between requests to avoid rate limiting
                await new Promise(resolve => setTimeout(resolve, 500));
              } catch (subChunkError) {
                console.error(`Error processing segment 5 sub-chunk ${chunkIndex}:`, subChunkError);
                // Continue with next sub-chunk instead of failing the whole segment
              }
            }
            
            // Mark success if we generated at least one audio chunk
            if (audioBuffers.length > chunkIndex - 1) {
              success = true;
              continue; // Skip the regular processing below since we handled this segment
            }
          } catch (segment5Error) {
            console.error("Error in special handling for segment 5:", segment5Error);
            // Fall through to regular processing as a backup
          }
        }
        
        // Implement retry logic for audio generation
        let retryCount = 0
        const maxRetries = 2
        let success = false
        let error = null
        
        console.log(`Generating audio for ${segment.speaker} segment ${i} with voice ${voice} (${cleanedSegmentText.length} chars)`)
        
        // Implement retry logic for audio generation
        let retryCount = 0
        const maxRetries = 2
        let success = false
        let error = null
        
        while (!success && retryCount <= maxRetries) {
          try {
            if (retryCount > 0) {
              console.log(`Retry ${retryCount}/${maxRetries} for segment ${i}`)
            }
            
            // Add a safety check for problematic content
            const safeText = cleanedSegmentText
              .replace(/[^\x20-\x7E\s]/g, "") // Remove non-printable ASCII characters
              .replace(/(\r\n|\n|\r)/gm, " ") // Replace newlines with spaces
              .replace(/\s+/g, " ") // Replace multiple spaces with single space
              .replace(/[""]/g, "\"") // Replace smart quotes with straight quotes
              .replace(/['']/g, "'") // Replace smart apostrophes with straight ones
              .replace(/[–—]/g, "-") // Replace em/en dashes with hyphens
              .replace(/[…]/g, "...") // Replace ellipsis character with periods
              .replace(/&/g, " and ") // Replace ampersands
              .trim()

            // Log safeText length and a small preview for debugging
            console.log(`Segment ${i} text length: ${safeText.length}, Preview: "${safeText.substring(0, 50)}${safeText.length > 50 ? '...' : ''}"`)
            
            // Split very long segments into smaller pieces if needed
            if (safeText.length > 3000) {
              // Find a good splitting point (period, question mark, exclamation)
              const midPoint = Math.floor(safeText.length / 2)
              const splitPoint = safeText.substring(midPoint).search(/[.!?]/) + midPoint;
              
              if (splitPoint > midPoint) {
                const firstHalf = safeText.substring(0, splitPoint + 1)
                const secondHalf = safeText.substring(splitPoint + 1)
                
                console.log(`Splitting long segment ${i} into two parts`)
                
                // Process first half
                const speechResponse1 = await openai.audio.speech.create({
                  model: "tts-1",
                  voice: voice,
                  input: firstHalf,
                })
                
                try {
                  const audioBuffer1 = await speechResponse1.arrayBuffer()
                  console.log(`Successfully decoded audio response for part 1 of segment ${i}, size: ${audioBuffer1.byteLength} bytes`)
                  audioBuffers.push(audioBuffer1)
                  audioMetadata.push({ speaker: segment.speaker, voice })
                } catch (decodeError) {
                  console.error(`Error decoding audio response for part 1 of segment ${i}:`, decodeError)
                  throw new Error(`Failed to decode audio response: ${decodeError.message}`)
                }
                
                // Process second half
                const speechResponse2 = await openai.audio.speech.create({
                  model: "tts-1",
                  voice: voice,
                  input: secondHalf,
                })
                
                try {
                  const audioBuffer2 = await speechResponse2.arrayBuffer()
                  console.log(`Successfully decoded audio response for part 2 of segment ${i}, size: ${audioBuffer2.byteLength} bytes`)
                  audioBuffers.push(audioBuffer2)
                  audioMetadata.push({ speaker: segment.speaker, voice })
                } catch (decodeError) {
                  console.error(`Error decoding audio response for part 2 of segment ${i}:`, decodeError)
                  throw new Error(`Failed to decode audio response: ${decodeError.message}`)
                }
              } else {
                // Fallback if we can't find a good split point
                const speechResponse = await openai.audio.speech.create({
                  model: "tts-1",
                  voice: voice,
                  input: safeText,
                })
                
                try {
                  const audioBuffer = await speechResponse.arrayBuffer()
                  console.log(`Successfully decoded audio response for segment ${i}, size: ${audioBuffer.byteLength} bytes`)
                  audioBuffers.push(audioBuffer)
                  audioMetadata.push({ speaker: segment.speaker, voice })
                } catch (decodeError) {
                  console.error(`Error decoding audio response for segment ${i}:`, decodeError)
                  throw new Error(`Failed to decode audio response: ${decodeError.message}`)
                }
              }
            } else {
              // Normal processing for standard-length segments
              const speechResponse = await openai.audio.speech.create({
                model: "tts-1",
                voice: voice,
                input: safeText,
              })
              
              try {
                const audioBuffer = await speechResponse.arrayBuffer()
                console.log(`Successfully decoded audio response for segment ${i}, size: ${audioBuffer.byteLength} bytes`)
                audioBuffers.push(audioBuffer)
                audioMetadata.push({ speaker: segment.speaker, voice })
              } catch (decodeError) {
                console.error(`Error decoding audio response for segment ${i}:`, decodeError)
                throw new Error(`Failed to decode audio response: ${decodeError.message}`)
              }
            }
            
            success = true
            
          } catch (err) {
            error = err
            console.error(`Error generating audio for segment ${i} (attempt ${retryCount+1}):`, err)
            
            // Log the full segment text for debugging purposes
            console.log(`Full text of segment ${i} that caused the error:`)
            console.log('---------------------- BEGIN SEGMENT TEXT ----------------------')
            console.log(cleanedSegmentText)
            console.log('----------------------- END SEGMENT TEXT -----------------------')
            
            // Also log the sanitized text that was actually sent to the API
            console.log(`Sanitized text sent to API:`)
            console.log('------------------- BEGIN SANITIZED TEXT ----------------------')
            console.log(safeText)
            console.log('-------------------- END SANITIZED TEXT -----------------------')
            
            retryCount++
            
            // Wait briefly before retrying
            if (retryCount <= maxRetries) {
              await new Promise(resolve => setTimeout(resolve, 1000 * retryCount))
            }
          }
        }
        
        // If we couldn't generate audio after retries, but we have some audio already,
        // we'll continue with what we have rather than failing the entire chunk
        if (!success && audioBuffers.length > 0) {
          console.warn(`Failed to generate audio for segment ${i} after ${maxRetries} retries, but continuing with ${audioBuffers.length} segments`)
        } else if (!success) {
          // More detailed error reporting
          let errorMsg = `Failed to generate audio for segment ${i} after ${maxRetries} retries`;
          if (error) {
            errorMsg += `: ${error.message || "Unknown error"}`;
            // Log more details about the error if available
            console.error(`Detailed error info for segment ${i}:`, {
              message: error.message,
              name: error.name,
              stack: error.stack,
            });
          }
          throw new Error(errorMsg);
        }
      }
      
      console.log(`Generated ${audioBuffers.length} audio segments for chunk ${nextChunkIndex}`)
      
      // Combine all audio segments into a single buffer
      if (audioBuffers.length === 0) {
        throw new Error("No audio segments were generated for this chunk")
      }
      
      // For now, we'll use a simple concatenation approach
      // In a more sophisticated implementation, we could use ffmpeg for better audio stitching
      const totalLength = audioBuffers.reduce((acc, buf) => acc + buf.byteLength, 0)
      const combinedBuffer = new Uint8Array(totalLength)
      
      let offset = 0
      for (let i = 0; i < audioBuffers.length; i++) {
        const buffer = audioBuffers[i]
        combinedBuffer.set(new Uint8Array(buffer), offset)
        offset += buffer.byteLength
        
        // Log which speaker and voice was used
        console.log(`Added segment ${i+1}: ${audioMetadata[i].speaker} (${audioMetadata[i].voice}), Length: ${buffer.byteLength} bytes`)
      }
      
      // Store the combined audio file
      const filePath = `chunks/${userId}/${jobId}/chunk_${nextChunkIndex}.mp3`
      
      const { error: uploadError } = await supabaseAdmin
        .storage
        .from('audio-files')
        .upload(filePath, combinedBuffer, {
          contentType: 'audio/mpeg',
          cacheControl: '3600'
        })
        
      if (uploadError) {
        throw new Error(`Failed to upload combined audio: ${uploadError.message}`)
      }
      
      // Get the public URL for the uploaded file
      const { data: { publicUrl } } = supabaseAdmin
        .storage
        .from('audio-files')
        .getPublicUrl(filePath)

      // Update chunk status to completed
      await supabaseAdmin
        .from('podcast_chunks')
        .update({
          status: 'completed',
          audio_url: publicUrl
        })
        .eq('job_id', jobId)
        .eq('chunk_index', nextChunkIndex)

      console.log(`Chunk ${nextChunkIndex} processed successfully with ${audioBuffers.length} segments`)

      // Check if we need to process next chunk
      const { data: updatedJobData } = await supabaseAdmin
        .from('podcast_jobs')
        .select('total_chunks, completed_chunks')
        .eq('id', jobId)
        .single()

      if (updatedJobData && updatedJobData.completed_chunks < updatedJobData.total_chunks - 1) {
        // Trigger processing of next chunk
        const nextNextChunkIndex = nextChunkIndex + 1

        // Get chunk count to make sure we're not out of bounds
        const { count } = await supabaseAdmin
          .from('podcast_chunks')
          .select('*', { count: 'exact', head: true })
          .eq('job_id', jobId)

        if (nextNextChunkIndex < count) {
          // Call this function again recursively for the next chunk
          const nextChunkUrl = `${supabaseUrl}/functions/v1/process-next-chunk`
          
          await fetch(nextChunkUrl, {
            method: 'POST',
            headers: {
              'Content-Type': 'application/json',
              'Authorization': `Bearer ${supabaseServiceKey}`
            },
            body: JSON.stringify({
              jobId: jobId,
              nextChunkIndex: nextNextChunkIndex
            })
          })
        } else {
          console.log(`All ${count} chunks have been processed or scheduled for job ${jobId}`)
        }
      } else {
        console.log(`All chunks have been processed for job ${jobId}`)
      }

      return new Response(
        JSON.stringify({
          success: true,
          message: `Successfully processed chunk ${nextChunkIndex} for job ${jobId}`
        }),
        {
          headers: { 'Content-Type': 'application/json' },
          status: 200,
        }
      )
    } catch (error) {
      console.error(`Error processing chunk ${nextChunkIndex}:`, error)
      
      // Update chunk status to error
      await supabaseAdmin
        .from('podcast_chunks')
        .update({
          status: 'error',
          error: error.message
        })
        .eq('job_id', jobId)
        .eq('chunk_index', nextChunkIndex)
      
      // Update job status to error
      await supabaseAdmin
        .from('podcast_jobs')
        .update({
          status: 'error',
          error: `Error processing chunk ${nextChunkIndex}: ${error.message}`
        })
        .eq('id', jobId)
      
      return new Response(
        JSON.stringify({
          success: false,
          error: `Error processing chunk ${nextChunkIndex}: ${error.message}`
        }),
        {
          headers: { 'Content-Type': 'application/json' },
          status: 500,
        }
      )
    }
  } catch (error) {
    console.error("Error in process-next-chunk function:", error)
    return new Response(
      JSON.stringify({
        success: false,
        error: error.message
      }),
      {
        headers: { 'Content-Type': 'application/json' },
        status: 500,
      }
    )
  }
}) 